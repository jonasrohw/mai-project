{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the datasets\n",
    "data_path = 'data/'\n",
    "\n",
    "# Load train data\n",
    "train_data = json.load(open(data_path + \"news_clippings/data/news_clippings/data/merged_balanced/train.json\"))\n",
    "train_data = pd.DataFrame(train_data[\"annotations\"])\n",
    "train_data.insert(0, 'new_clipping_id', range(0, len(train_data)))\n",
    "train_data.columns.values[1] = 'article_id'\n",
    "\n",
    "\n",
    "# Load VisualNews data\n",
    "vn_data = json.load(open(data_path + '/VisualNews/origin/data.json'))\n",
    "vn_data = pd.DataFrame(vn_data)\n",
    "vn_data = vn_data[['id', 'image_path', 'article_path']]\n",
    "\n",
    "\n",
    "# Load source evidence paths\n",
    "SOURCE_EVIDENCE_PATH = 'data/news_clippings/queries_dataset'\n",
    "\n",
    "train_paths = pd.DataFrame(json.load(open(SOURCE_EVIDENCE_PATH + '/dataset_items_train.json'))).transpose()\n",
    "train_paths = train_paths.reset_index().rename(columns={'index': 'match_index'})\n",
    "train_paths['match_index'] = train_paths['match_index'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "merged_train_data = pd.merge(train_data, train_paths, left_on='new_clipping_id', right_on='match_index')\n",
    "\n",
    "merged_with_article_data = pd.merge(merged_train_data, vn_data, left_on='article_id', right_on='id', how='left')\n",
    "merged_with_article_data = merged_with_article_data.rename(columns={'image_path': 'article_id_image_path', 'article_path': 'article_id_article_path'})\n",
    "\n",
    "# Merge the resulting data with vn_data on image_id to get article_path and image_path\n",
    "final_merged_data = pd.merge(merged_with_article_data, vn_data, left_on='image_id', right_on='id', how='left')\n",
    "final_merged_data = final_merged_data.rename(columns={'image_path': 'image_id_image_path', 'article_path': 'image_id_article_path'})\n",
    "\n",
    "\n",
    "### Let us delete 9/10\n",
    "num_entries_to_keep = len(final_merged_data) // 10\n",
    "subset_final_merged_data = final_merged_data.head(num_entries_to_keep)\n",
    "last_new_clipping_id = subset_final_merged_data['new_clipping_id'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotations after the specified id have been deleted.\n"
     ]
    }
   ],
   "source": [
    "### Update news_clippings\n",
    "\n",
    "# relative path: data/news_clippings/data/news_clippings/data/merged_balanced/train.json\n",
    "with open(data_path + \"news_clippings/data/news_clippings/data/merged_balanced/train.json\", 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Find the index of the last specified id\n",
    "# Remove all annotations after the specified id\n",
    "data['annotations'] = data['annotations'][:last_new_clipping_id + 1]\n",
    "# Save the modified JSON data back to the file\n",
    "with open(data_path + \"news_clippings/data/news_clippings/data/merged_balanced/train.json\", 'w') as file:\n",
    "    json.dump(data, file)\n",
    "\n",
    "print(\"Annotations after the specified id have been deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_merge_data(data_path, dataset_type):\n",
    "    # Load the dataset\n",
    "    dataset = json.load(open(f\"{data_path}/news_clippings/data/news_clippings/data/merged_balanced/{dataset_type}.json\"))\n",
    "    dataset = pd.DataFrame(dataset[\"annotations\"])\n",
    "    dataset.insert(0, 'new_clipping_id', range(0, len(dataset)))\n",
    "    dataset.columns.values[1] = 'article_id'\n",
    "\n",
    "\n",
    "    # Load VisualNews data\n",
    "    vn_data = json.load(open(data_path + '/VisualNews/origin/data.json'))\n",
    "    vn_data = pd.DataFrame(vn_data)\n",
    "    vn_data = vn_data[['id', 'image_path', 'article_path']]\n",
    "\n",
    "    # Merge datasets\n",
    "    merged_with_article_data = pd.merge(dataset, vn_data, left_on='article_id', right_on='id', how='left')\n",
    "    merged_with_article_data = merged_with_article_data.rename(columns={'image_path': 'article_id_image_path', 'article_path': 'article_id_article_path'})\n",
    "\n",
    "    # Merge the resulting data with vn_data on image_id to get article_path and image_path\n",
    "    final_merged_data = pd.merge(merged_with_article_data, vn_data, left_on='image_id', right_on='id', how='left')\n",
    "    final_merged_data = final_merged_data.rename(columns={'image_path': 'image_id_image_path', 'article_path': 'image_id_article_path'})\n",
    "    \n",
    "    return final_merged_data\n",
    "\n",
    "\n",
    "data_path = 'data/'\n",
    "val_data = load_and_merge_data(data_path, 'val')\n",
    "test_data = load_and_merge_data(data_path, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19018\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Extract article_ids and image_ids from the validation and test datasets\n",
    "article_ids_val = val_data['id_x']\n",
    "image_ids_val = val_data['id_y']\n",
    "article_ids_test = test_data['id_x']\n",
    "image_ids_test = test_data['id_y']\n",
    "\n",
    "# Extract article_ids from the train dataset to protect them\n",
    "article_ids_train = subset_final_merged_data['id_x']\n",
    "image_ids_train = subset_final_merged_data['id_y']\n",
    "\n",
    "# Combine the IDs and remove duplicates\n",
    "all_ids = pd.concat([article_ids_val, image_ids_val, article_ids_test, image_ids_test, article_ids_train, image_ids_train]).unique()\n",
    "\n",
    "print(len(all_ids))\n",
    "\n",
    "# Load the main JSON file\n",
    "with open('data/VisualNews/origin/data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "ids_to_keep = all_ids\n",
    "\n",
    "# Convert ids_to_keep to a set for faster lookups\n",
    "ids_to_keep_set = set(ids_to_keep)\n",
    "\n",
    "# Define the base path for images\n",
    "base_image_path = \"data/VisualNews/origin\"\n",
    "\n",
    "# Filter the data to only include items with IDs in the ids_to_keep list\n",
    "filtered_data = []\n",
    "for item in data:\n",
    "    if item['id'] in ids_to_keep_set:\n",
    "        filtered_data.append(item)\n",
    "    else:\n",
    "        # Delete associated files\n",
    "        image_path = item.get('image_path')\n",
    "        article_path = item.get('article_path')\n",
    "        \n",
    "        if image_path:\n",
    "            full_image_path = os.path.join(base_image_path, image_path.lstrip('./'))\n",
    "            if os.path.isfile(full_image_path):\n",
    "                try:\n",
    "                    os.remove(full_image_path)\n",
    "                    print(f\"Deleted file: {full_image_path}\")\n",
    "                except OSError as e:\n",
    "                    print(f\"Error deleting file {full_image_path}: {e}\")\n",
    "\n",
    "        if article_path and os.path.isfile(article_path):\n",
    "            try:\n",
    "                os.remove(article_path)\n",
    "                print(f\"Deleted file: {article_path}\")\n",
    "            except OSError as e:\n",
    "                print(f\"Error deleting file {article_path}: {e}\")\n",
    "\n",
    "# Save the filtered data back to a new JSON file\n",
    "with open('data/VisualNews/origin/data.json', 'w') as file:\n",
    "    json.dump(filtered_data, file)\n",
    "\n",
    "print(f\"Filtered data saved to 'filtered_large_file.json'. Original count: {len(data)}, Filtered count: {len(filtered_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Load the dataset\n",
    "with open('data/news_clippings/queries_dataset/dataset_items_train.json', 'r') as infile:\n",
    "    data = json.load(infile)\n",
    "\n",
    "last_id = int(last_new_clipping_id)\n",
    "new_data = {}\n",
    "\n",
    "# Define the base path for the directories\n",
    "base_directory_path = \"data/news_clippings/queries_dataset/merged_balanced\"\n",
    "\n",
    "# Process the data and delete directories beyond last_id\n",
    "for key in list(data.keys()):\n",
    "    key_int = int(key)\n",
    "    if key_int <= last_id:\n",
    "        new_data[key] = data[key]\n",
    "    else:\n",
    "        # Delete the directories\n",
    "        if 'inv_path' in data[key]:\n",
    "            full_inv_path = os.path.join(base_directory_path, data[key]['inv_path'].lstrip('./'))\n",
    "            if os.path.exists(full_inv_path):\n",
    "                try:\n",
    "                    shutil.rmtree(full_inv_path)\n",
    "                    print(f\"Deleted directory: {full_inv_path}\")\n",
    "                except OSError as e:\n",
    "                    print(f\"Error deleting directory {full_inv_path}: {e}\")\n",
    "\n",
    "        if 'direct_path' in data[key]:\n",
    "            full_direct_path = os.path.join(base_directory_path, data[key]['direct_path'].lstrip('./'))\n",
    "            if os.path.exists(full_direct_path):\n",
    "                try:\n",
    "                    shutil.rmtree(full_direct_path)\n",
    "                    print(f\"Deleted directory: {full_direct_path}\")\n",
    "                except OSError as e:\n",
    "                    print(f\"Error deleting directory {full_direct_path}: {e}\")\n",
    "\n",
    "# Save the new data back to the file\n",
    "with open('data/news_clippings/queries_dataset/dataset_items_train.json', 'w') as outfile:\n",
    "    json.dump(new_data, outfile)\n",
    "\n",
    "print(f\"Updated data saved to 'data/news_clippings/queries_dataset/dataset_items_train.json'. Original count: {len(data)}, New count: {len(new_data)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
